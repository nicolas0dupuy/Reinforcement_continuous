{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import agent\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like:\n",
      " [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "path_to_env='../Reacher_Linux/Reacher.x86_64'\n",
    "\n",
    "env = UnityEnvironment(file_name=path_to_env)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:\\n', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "# Tests for memory usage\n",
    "#myAgent = agent.Agent(3)\n",
    "#l = myAgent.memory_size\n",
    "\n",
    "for i in range(20):\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actionsClipped = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    print(actionsClipped)\n",
    "    env_info = env.step(actionsClipped)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    \n",
    "    myAgent.add(states[0], actions[0], rewards[0], next_states[0], dones[0])\n",
    "    for N, j in enumerate(rB.episode_len):\n",
    "        l_now = len(myAgent.individual_memory)\n",
    "        if ( l_now >= j):\n",
    "            past_states = [myAgent.individual_memory[k].state for k in range(l_now-j,l_now)]\n",
    "            past_actions = [myAgent.individual_memory[k].action for k in range(l_now-j,l_now)]\n",
    "            past_rewards = [myAgent.individual_memory[k].reward for k in range(l_now-j,l_now)]\n",
    "            past_next_states = [myAgent.individual_memory[k].next_state for k in range(l_now-j,l_now)]\n",
    "            past_dones = [myAgent.individual_memory[k].done for k in range(l_now-j,l_now)]\n",
    "            rB.add(past_states, past_actions, past_rewards, past_next_states, past_dones, N)\n",
    "    \n",
    "    states = next_states                               # roll over states to next time step\n",
    "    #if np.any(dones):                                  # exit loop if episode finished\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start learning\n",
      "Total score (averaged over agents) for episode 0: 0.0019999999552965165\n",
      "Total score (averaged over agents) for episode 1: 0.06399999856948853\n",
      "Total score (averaged over agents) for episode 2: 0.008999999798834323\n",
      "Total score (averaged over agents) for episode 3: 0.0\n",
      "Total score (averaged over agents) for episode 4: 0.0029999999329447745\n",
      "Total score (averaged over agents) for episode 5: 0.0\n",
      "Total score (averaged over agents) for episode 6: 0.0019999999552965165\n",
      "Total score (averaged over agents) for episode 7: 0.0034999999217689036\n",
      "Total score (averaged over agents) for episode 8: 0.0\n",
      "Total score (averaged over agents) for episode 9: 0.006999999843537807\n",
      "Total score (averaged over agents) for episode 10: 0.013999999687075614\n",
      "Total score (averaged over agents) for episode 11: 0.0\n",
      "Total score (averaged over agents) for episode 12: 0.02149999951943755\n",
      "Total score (averaged over agents) for episode 13: 0.0\n",
      "Total score (averaged over agents) for episode 14: 0.0064999998547136785\n",
      "Total score (averaged over agents) for episode 15: 0.013999999687075614\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-94dc386343c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;31m#train the actors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mworker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                     \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExpstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states\u001b[0m                               \u001b[0;31m# roll over states to next time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                                  \u001b[0;31m# exit loop if episode finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/nicolas-dupuy/Seagate Backup Plus Drive/Udacity_lessons/deep-reinforcement-learning/p2_continuous-control/Reinforcement_continuous/agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mactor_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msoft_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/drlnd/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_critics = 1 # Define the number of independant critics wanted\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "\n",
    "# Penalty coefficient for high actions\n",
    "alphaTor = 0.0000025\n",
    "\n",
    "\n",
    "# Create Critic Agents\n",
    "critics = [] # agent list\n",
    "crit_lrs = 1e-4 # np.exp(-2* np.log(10) * np.random.rand(20)) * 10**(-2)\n",
    "for i in range(num_critics):\n",
    "    model = agent.CriticModel(state_size, action_size)\n",
    "    critic = agent.CriticAgent(model, crit_lrs)\n",
    "    critics.append(critic)\n",
    "\n",
    "# Create Actor Agents\n",
    "workers = [] # agent list\n",
    "crit2work = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "# work_lrs = [1e-4,1e-3,1e-5,2e-4,2e-3,1e-4,2e-5,3e-4,3e-5,3e-4,2e-5,1e-3,1e-4,1e-5,2e-4,3e-4,3e-3,2e-5,5e-5,5e-4]\n",
    "for i in range(num_agents):\n",
    "    model = agent.ActorModel(state_size, action_size)\n",
    "    critic = critics[0] #crit2work[i]]\n",
    "    actor = agent.ActorAgent(model, critic, lr = 1e-4,memory_size = 3)\n",
    "    actor.noise.theta = 0.2 # set theta in OUnoise\n",
    "    actor.noise.sigma = 0.2 # set sigma in OUnoise\n",
    "    workers.append(actor)\n",
    "    \n",
    "\n",
    "# Create the shared replay buffer\n",
    "shared_memory = agent.ReplayBuffer(action_size, buffer_size = 1e5, batch_size =512, episode_len=[1])\n",
    "scoreList = []\n",
    "count = 0\n",
    "start_learning = True # to identify when the agents start learning\n",
    "while True:\n",
    "    scores = np.zeros(num_agents)\n",
    "    while True:\n",
    "        # Select an action for each agent\n",
    "        actions = []\n",
    "        for i, worker in enumerate(workers):\n",
    "            state = states[i]\n",
    "            action = worker.act(state)\n",
    "            actions.append(action)\n",
    "\n",
    "        # send the actions to the environment\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "\n",
    "        # Memorize last actions\n",
    "        for i, worker in enumerate(workers):\n",
    "            # add a penalty for excessive torque (high action values)\n",
    "            myReward = rewards[i] # - alphaTor * np.abs(actions[i].max())\n",
    "            worker.add(states[i], actions[i], myReward, next_states[i], dones[i])\n",
    "        # send individual memory to shared memory\n",
    "        for spot, spotLength in enumerate(shared_memory.episode_len):\n",
    "            for worker in workers:\n",
    "                l = len(worker.individual_memory)\n",
    "                if ( l >= spotLength): # check if the agent has enough steps in memory for the current shared memory spot\n",
    "                    currentRange = range(l-spotLength, l)\n",
    "                    past_states = [worker.individual_memory[k].state for k in currentRange]\n",
    "                    past_actions = [worker.individual_memory[k].action for k in currentRange]\n",
    "                    past_rewards = [worker.individual_memory[k].reward for k in currentRange]\n",
    "                    past_next_states = [worker.individual_memory[k].next_state for k in currentRange]\n",
    "                    past_dones = [worker.individual_memory[k].done for k in currentRange]\n",
    "                    shared_memory.add(past_states, past_actions, past_rewards, past_next_states, past_dones, spot)\n",
    "        # Train the critics\n",
    "        # 1) generate a sample with the various length replaybuffers (use ReplayBuffer.sample and CriticAgent.evaluate)\n",
    "        for i in range(len(shared_memory.episode_len)): # loop over the different spots\n",
    "            if (len(shared_memory) > shared_memory.batch_size):\n",
    "                if start_learning:\n",
    "                    print(\"start learning\")\n",
    "                    start_learning = False\n",
    "                Expstates, Expactions, Exprewards, Expnext_states, Expdones = shared_memory.sample(i) # sample the ith memory spot\n",
    "            # 2) critic.learn\n",
    "                for i, critic in enumerate(critics): # adapt for long episodes\n",
    "                    # Expnext_state = Expnext_states[-1]\n",
    "                    Expnext_actions = workers[i].act(Expnext_states)\n",
    "                    Expstate, Expaction, Expestimated_reward = critic.evaluate(Expstates.squeeze(), Expactions.squeeze(), Exprewards, Expnext_states.squeeze(), Expnext_actions.squeeze(), Expdones.squeeze())\n",
    "                    critic.learn(Expstate, Expaction, Expestimated_reward)\n",
    "                #train the actors\n",
    "                for worker in workers:\n",
    "                    worker.learn(Expstate)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    if True: # count%10 == 0:\n",
    "        print('Total score (averaged over agents) for episode {}: {}'.format(count ,np.mean(scores)))\n",
    "    scoreList.append([scores])\n",
    "    count += 1\n",
    "    if np.mean(np.array(scoreList)[-100:]) > 30:\n",
    "        break\n",
    "print(\"Total mean score: {}\".format(np.mean(scoreList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.3665225 , 0.63007367, 0.47091824, 0.44003147]],\n",
       "\n",
       "       [[0.3665225 , 0.63007367, 0.47091824, 0.44003147]],\n",
       "\n",
       "       [[0.3665225 , 0.63007367, 0.47091824, 0.44003147]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.3665225 , 0.63007367, 0.47091824, 0.44003147]],\n",
       "\n",
       "       [[0.3665225 , 0.63007367, 0.47091824, 0.44003147]],\n",
       "\n",
       "       [[0.3665225 , 0.63007367, 0.47091824, 0.44003147]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Expstates, Expactions, Exprewards, Expnext_states, Expdones = shared_memory.sample(0)\n",
    "Expnext_actions = workers[0].act(Expnext_states)\n",
    "Expstate, Expaction, Expestimated_reward = critic.evaluate(Expstates.squeeze(), Expactions.squeeze(), Exprewards, Expnext_states.squeeze(), Expnext_actions.squeeze(), Expdones.squeeze())\n",
    "#Expnext_states.squeeze().shape\n",
    "Expnext_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.1986014e-37,  2.6169830e-28,  1.7499664e-31, -2.1355674e-31]],\n",
       "\n",
       "       [[ 1.1986014e-37,  2.6169830e-28,  1.7499664e-31, -2.1355674e-31]],\n",
       "\n",
       "       [[ 1.1986014e-37,  2.6169830e-28,  1.7499664e-31, -2.1355674e-31]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.1986014e-37,  2.6169830e-28,  1.7499664e-31, -2.1355674e-31]],\n",
       "\n",
       "       [[ 1.1986014e-37,  2.6169830e-28,  1.7499664e-31, -2.1355674e-31]],\n",
       "\n",
       "       [[ 1.1986014e-37,  2.6169830e-28,  1.7499664e-31, -2.1355674e-31]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO FIX! \n",
    "worker.act(Expstates[:], add_noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "critic.target(torch.from_numpy(Expnext_states.squeeze()).float().to(\"cuda\"), torch.from_numpy(Expnext_actions.squeeze()).float().to(\"cuda\")).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for i in range(20):\n",
    "    x = np.array(scoreList)[:,:,i]\n",
    "    plt.plot(range(len(x)), x, label=i)\n",
    "    #plt.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plt.plot(np.array(shared_memory.memory[0])[:,2,0])\n",
    "np.sum((np.array(shared_memory.memory[0])[:,2,0]) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers[0].noise.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
