{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import agent\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like:\n",
      " [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "path_to_env='../Reacher_Linux/Reacher.x86_64'\n",
    "\n",
    "env = UnityEnvironment(file_name=path_to_env)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:\\n', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "# Tests for memory usage\n",
    "#myAgent = agent.Agent(3)\n",
    "#l = myAgent.memory_size\n",
    "\n",
    "for i in range(20):\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actionsClipped = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    print(actionsClipped)\n",
    "    env_info = env.step(actionsClipped)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    \n",
    "    myAgent.add(states[0], actions[0], rewards[0], next_states[0], dones[0])\n",
    "    for N, j in enumerate(rB.episode_len):\n",
    "        l_now = len(myAgent.individual_memory)\n",
    "        if ( l_now >= j):\n",
    "            past_states = [myAgent.individual_memory[k].state for k in range(l_now-j,l_now)]\n",
    "            past_actions = [myAgent.individual_memory[k].action for k in range(l_now-j,l_now)]\n",
    "            past_rewards = [myAgent.individual_memory[k].reward for k in range(l_now-j,l_now)]\n",
    "            past_next_states = [myAgent.individual_memory[k].next_state for k in range(l_now-j,l_now)]\n",
    "            past_dones = [myAgent.individual_memory[k].done for k in range(l_now-j,l_now)]\n",
    "            rB.add(past_states, past_actions, past_rewards, past_next_states, past_dones, N)\n",
    "    \n",
    "    states = next_states                               # roll over states to next time step\n",
    "    #if np.any(dones):                                  # exit loop if episode finished\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) for episode 0: 0.30549999317154286\n",
      "Total score (averaged over agents) for episode 1: 0.0549999987706542\n",
      "Total score (averaged over agents) for episode 2: 0.3674999917857349\n",
      "Total score (averaged over agents) for episode 3: 0.040499999094754456\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9ff2cc9b77f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0mExpnext_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExpnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0mExpstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpaction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpestimated_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExpstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExprewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpnext_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpnext_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpdones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExpstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpaction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpestimated_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;31m#train the actors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mworker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/nicolas-dupuy/Seagate Backup Plus Drive/Udacity_lessons/deep-reinforcement-learning/p2_continuous-control/Reinforcement_continuous/agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, states, actions, estimated_rewards, lrFactor)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# Minimize the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;31m# optimize with learning rate adapted by lrFactor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# self.optimizer.lr *= lrFactor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/drlnd/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/drlnd/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_critics = 1 # Define the number of independant critics wanted\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "\n",
    "# Penalty coefficient for high actions\n",
    "alphaTor = 0.005\n",
    "\n",
    "\n",
    "# Create Critic Agents\n",
    "critics = [] # agent list\n",
    "crit_lrs = 1e-4 # np.exp(-2* np.log(10) * np.random.rand(20)) * 10**(-2)\n",
    "for i in range(num_critics):\n",
    "    model = agent.CriticModel(state_size, action_size)\n",
    "    critic = agent.CriticAgent(model, crit_lrs)\n",
    "    critics.append(critic)\n",
    "\n",
    "# Create Actor Agents\n",
    "workers = [] # agent list\n",
    "crit2work = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "# work_lrs = [1e-4,1e-3,1e-5,2e-4,2e-3,1e-4,2e-5,3e-4,3e-5,3e-4,2e-5,1e-3,1e-4,1e-5,2e-4,3e-4,3e-3,2e-5,5e-5,5e-4]\n",
    "for i in range(num_agents):\n",
    "    model = agent.ActorModel(state_size, action_size)\n",
    "    critic = critics[0] #crit2work[i]]\n",
    "    actor = agent.ActorAgent(model, critic, lr = 1e-4,memory_size = 3)\n",
    "    actor.noise.theta = 0.1 # set theta in OUnoise\n",
    "    actor.noise.sigma = 0.1 # set sigma in OUnoise\n",
    "    workers.append(actor)\n",
    "    \n",
    "\n",
    "# Create the shared replay buffer\n",
    "shared_memory = agent.ReplayBuffer(action_size, buffer_size = 1e5, batch_size =512, episode_len=[1])\n",
    "scoreList = []\n",
    "count = 0\n",
    "while True:\n",
    "    scores = np.zeros(num_agents)\n",
    "    while True:\n",
    "        # Select an action for each agent\n",
    "        actions = []\n",
    "        for i, worker in enumerate(workers):\n",
    "            state = states[i]\n",
    "            action = worker.act(state)\n",
    "            actions.append(action)\n",
    "\n",
    "        # send the actions to the environment\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "\n",
    "        # Memorize last actions\n",
    "        for i, worker in enumerate(workers):\n",
    "            # add a penalty for excessive torque (high action values)\n",
    "            myReward = rewards[i] - alphaTor * np.abs(actions[i].max())\n",
    "            worker.add(states[i], actions[i], myReward, next_states[i], dones[i])\n",
    "        # send individual memory to shared memory\n",
    "        for spot, spotLength in enumerate(shared_memory.episode_len):\n",
    "            for worker in workers:\n",
    "                l = len(worker.individual_memory)\n",
    "                if ( l >= spotLength): # check if the agent has enough steps in memory for the current shared memory spot\n",
    "                    currentRange = range(l-spotLength, l)\n",
    "                    past_states = [worker.individual_memory[k].state for k in currentRange]\n",
    "                    past_actions = [worker.individual_memory[k].action for k in currentRange]\n",
    "                    past_rewards = [worker.individual_memory[k].reward for k in currentRange]\n",
    "                    past_next_states = [worker.individual_memory[k].next_state for k in currentRange]\n",
    "                    past_dones = [worker.individual_memory[k].done for k in currentRange]\n",
    "                    shared_memory.add(past_states, past_actions, past_rewards, past_next_states, past_dones, spot)\n",
    "        # Train the critics\n",
    "        # 1) generate a sample with the various length replaybuffers (use ReplayBuffer.sample and CriticAgent.evaluate)\n",
    "        for i in range(len(shared_memory.episode_len)): # loop over the different spots\n",
    "            if (len(shared_memory) > shared_memory.batch_size):\n",
    "                Expstates, Expactions, Exprewards, Expnext_states, Expdones = shared_memory.sample(i) # sample the ith memory spot\n",
    "            # 2) critic.learn\n",
    "                for i, critic in enumerate(critics): # adapt for long episodes\n",
    "                    # Expnext_state = Expnext_states[-1]\n",
    "                    Expnext_actions = workers[i].act(Expnext_states)\n",
    "                    Expstate, Expaction, Expestimated_reward = critic.evaluate(Expstates.squeeze(), Expactions.squeeze(), Exprewards, Expnext_states.squeeze(), Expnext_actions.squeeze(), Expdones.squeeze())\n",
    "                    critic.learn(Expstate, Expaction, Expestimated_reward)\n",
    "                #train the actors\n",
    "                for worker in workers:\n",
    "                    worker.learn(Expstate)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    if True: # count%10 == 0:\n",
    "        print('Total score (averaged over agents) for episode {}: {}'.format(count ,np.mean(scores)))\n",
    "    scoreList.append([scores])\n",
    "    count += 1\n",
    "    if np.mean(np.array(scoreList)[-100:]) > 30:\n",
    "        break\n",
    "print(\"Total mean score: {}\".format(np.mean(scoreList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expstates, Expactions, Exprewards, Expnext_states, Expdones = shared_memory.sample(0)\n",
    "Expnext_actions = workers[i].act(Expnext_states)\n",
    "Expstate, Expaction, Expestimated_reward = critic.evaluate(Expstates.squeeze(), Expactions.squeeze(), Exprewards, Expnext_states.squeeze(), Expnext_actions.squeeze(), Expdones.squeeze())\n",
    "#Expnext_states.squeeze().shape\n",
    "Expestimated_reward.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "critic.target(torch.from_numpy(Expnext_states.squeeze()).float().to(\"cuda\"), torch.from_numpy(Expnext_actions.squeeze()).float().to(\"cuda\")).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for i in range(3):\n",
    "    plt.plot( np.array(scoreList)[:,:,i], label=i)\n",
    "    plt.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_memory.memory[0][-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers[0].noise.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
