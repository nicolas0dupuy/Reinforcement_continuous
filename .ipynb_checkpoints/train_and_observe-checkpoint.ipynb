{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import agent\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like:\n",
      " [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "path_to_env='../Reacher_Linux/Reacher.x86_64'\n",
    "\n",
    "env = UnityEnvironment(file_name=path_to_env)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:\\n', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "# Tests for memory usage\n",
    "#myAgent = agent.Agent(3)\n",
    "#l = myAgent.memory_size\n",
    "\n",
    "for i in range(20):\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actionsClipped = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    print(actionsClipped)\n",
    "    env_info = env.step(actionsClipped)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    \n",
    "    myAgent.add(states[0], actions[0], rewards[0], next_states[0], dones[0])\n",
    "    for N, j in enumerate(rB.episode_len):\n",
    "        l_now = len(myAgent.individual_memory)\n",
    "        if ( l_now >= j):\n",
    "            past_states = [myAgent.individual_memory[k].state for k in range(l_now-j,l_now)]\n",
    "            past_actions = [myAgent.individual_memory[k].action for k in range(l_now-j,l_now)]\n",
    "            past_rewards = [myAgent.individual_memory[k].reward for k in range(l_now-j,l_now)]\n",
    "            past_next_states = [myAgent.individual_memory[k].next_state for k in range(l_now-j,l_now)]\n",
    "            past_dones = [myAgent.individual_memory[k].done for k in range(l_now-j,l_now)]\n",
    "            rB.add(past_states, past_actions, past_rewards, past_next_states, past_dones, N)\n",
    "    \n",
    "    states = next_states                               # roll over states to next time step\n",
    "    #if np.any(dones):                                  # exit loop if episode finished\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start learning\n",
      "Total score (averaged over agents) for episode 0: 0.21349999522790314\n",
      "Total score (averaged over agents) for episode 1: 0.6014999865554274\n",
      "Total score (averaged over agents) for episode 2: 1.1944999733008443\n",
      "Total score (averaged over agents) for episode 3: 0.7499999832361937\n",
      "Total score (averaged over agents) for episode 4: 0.9389999790117145\n",
      "Total score (averaged over agents) for episode 5: 1.2184999727644026\n",
      "Total score (averaged over agents) for episode 6: 0.8174999817274511\n",
      "Total score (averaged over agents) for episode 7: 0.4084999908693135\n",
      "Total score (averaged over agents) for episode 8: 0.6449999855831265\n",
      "Total score (averaged over agents) for episode 9: 0.7199999839067459\n",
      "Total score (averaged over agents) for episode 10: 0.8284999814815819\n",
      "Total score (averaged over agents) for episode 11: 0.5284999881871044\n",
      "Total score (averaged over agents) for episode 12: 0.677999984845519\n",
      "Total score (averaged over agents) for episode 13: 0.7489999832585454\n",
      "Total score (averaged over agents) for episode 14: 0.15949999643489718\n",
      "Total score (averaged over agents) for episode 15: 0.39799999110400675\n",
      "Total score (averaged over agents) for episode 16: 0.5774999870918691\n",
      "Total score (averaged over agents) for episode 17: 0.5269999882206321\n",
      "Total score (averaged over agents) for episode 18: 0.3954999911598861\n",
      "Total score (averaged over agents) for episode 19: 0.3449999922886491\n",
      "Total score (averaged over agents) for episode 20: 0.3434999923221767\n",
      "Total score (averaged over agents) for episode 21: 0.3714999916963279\n",
      "Total score (averaged over agents) for episode 22: 0.5779999870806932\n",
      "Total score (averaged over agents) for episode 23: 0.7339999835938216\n",
      "Total score (averaged over agents) for episode 24: 0.5514999876730144\n",
      "Total score (averaged over agents) for episode 25: 0.6109999863430857\n",
      "Total score (averaged over agents) for episode 26: 0.801499982085079\n",
      "Total score (averaged over agents) for episode 27: 0.6684999850578606\n",
      "Total score (averaged over agents) for episode 28: 0.739499983470887\n"
     ]
    }
   ],
   "source": [
    "num_critics = 1 # Define the number of independant critics wanted\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "\n",
    "# Penalty coefficient for high actions\n",
    "# alphaTor = 0.0000025\n",
    "\n",
    "\n",
    "# Create Critic Agents\n",
    "critics = [] # agent list\n",
    "crit_lrs = 2.5e-4 # np.exp(-2* np.log(10) * np.random.rand(20)) * 10**(-2)\n",
    "for i in range(num_critics):\n",
    "    model = agent.CriticModel(state_size, action_size)\n",
    "    critic = agent.CriticAgent(model, crit_lrs)\n",
    "    critics.append(critic)\n",
    "\n",
    "# Create Actor Agents\n",
    "workers = [] # agent list\n",
    "crit2work = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "# work_lrs = [1e-4,1e-3,1e-5,2e-4,2e-3,1e-4,2e-5,3e-4,3e-5,3e-4,2e-5,1e-3,1e-4,1e-5,2e-4,3e-4,3e-3,2e-5,5e-5,5e-4]\n",
    "for i in range(num_agents):\n",
    "    model = agent.ActorModel(state_size, action_size)\n",
    "    critic = critics[0] #crit2work[i]]\n",
    "    actor = agent.ActorAgent(model, critic, lr = 2.5e-4,memory_size = 1)\n",
    "    actor.noise.theta = 0.2 # set theta in OUnoise\n",
    "    actor.noise.sigma = 0.2 # set sigma in OUnoise\n",
    "    workers.append(actor)\n",
    "    \n",
    "\n",
    "# Create the shared replay buffer\n",
    "shared_memory = agent.ReplayBuffer(action_size, buffer_size = 4e4, batch_size = 512, episode_len=[1])\n",
    "scoreList = []\n",
    "count = 0\n",
    "start_learning = True # to identify when the agents start learning\n",
    "while True:\n",
    "    scores = np.zeros(num_agents)\n",
    "    learning_count =0 # To learn every 4 steps\n",
    "    while True:\n",
    "        # Select an action for each agent\n",
    "        actions = []\n",
    "        for i, worker in enumerate(workers):\n",
    "            state = states[i]\n",
    "            action = worker.act(state)\n",
    "            actions.append(action)\n",
    "\n",
    "        # send the actions to the environment\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "\n",
    "        # Memorize last actions\n",
    "        for i, worker in enumerate(workers):\n",
    "            # add a penalty for excessive torque (high action values)\n",
    "            myReward = rewards[i] # - alphaTor * np.abs(actions[i].max())\n",
    "            worker.add(states[i], actions[i], myReward, next_states[i], dones[i])\n",
    "        # send individual memory to shared memory\n",
    "        for spot, spotLength in enumerate(shared_memory.episode_len):\n",
    "            for worker in workers:\n",
    "                l = len(worker.individual_memory)\n",
    "                if ( l >= spotLength): # check if the agent has enough steps in memory for the current shared memory spot\n",
    "                    currentRange = range(l-spotLength, l)\n",
    "                    past_states = [worker.individual_memory[k].state for k in currentRange]\n",
    "                    past_actions = [worker.individual_memory[k].action for k in currentRange]\n",
    "                    past_rewards = [worker.individual_memory[k].reward for k in currentRange]\n",
    "                    past_next_states = [worker.individual_memory[k].next_state for k in currentRange]\n",
    "                    past_dones = [worker.individual_memory[k].done for k in currentRange]\n",
    "                    shared_memory.add(past_states, past_actions, past_rewards, past_next_states, past_dones, spot)\n",
    "        # Train\n",
    "        # 1) generate a sample with the various length replaybuffers\n",
    "        learning_count = (learning_count + 1) % 3\n",
    "        for j in range(len(shared_memory.episode_len)): # loop over the different spots\n",
    "            if (len(shared_memory) > shared_memory.batch_size) and (learning_count == 0):\n",
    "                if start_learning:\n",
    "                    print(\"start learning\")\n",
    "                    start_learning = False\n",
    "                \n",
    "            # 2) critic.learn\n",
    "                for i, critic in enumerate(critics): # adapt for long episodes\n",
    "                    Expstates, Expactions, Exprewards, Expnext_states, Expdones = shared_memory.sample(j) # sample the ith memory spot\n",
    "                    # Expnext_state = Expnext_states[-1]\n",
    "                    Expnext_actions = workers[i].act(Expnext_states, add_noise=False)\n",
    "                    Expstate, Expaction, Expestimated_reward = critic.evaluate(Expstates.squeeze(), Expactions.squeeze(), Exprewards, Expnext_states.squeeze(), Expnext_actions.squeeze(), Expdones.squeeze())\n",
    "                    critic.learn(Expstate, Expaction, Expestimated_reward)\n",
    "                #train the actors\n",
    "                # for worker in workers:\n",
    "                for i, worker in enumerate(workers):\n",
    "                    Expstates, Expactions, Exprewards, Expnext_states, Expdones = shared_memory.sample(j) # sample the ith memory spot\n",
    "                    # Expnext_state = Expnext_states[-1]\n",
    "                    Expnext_actions = workers[i].act(Expnext_states, add_noise=False)\n",
    "                    Expstate, Expaction, Expestimated_reward = critics[0].evaluate(Expstates.squeeze(), Expactions.squeeze(), Exprewards, Expnext_states.squeeze(), Expnext_actions.squeeze(), Expdones.squeeze())\n",
    "                    worker.learn(Expstate)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    if True: # count%10 == 0:\n",
    "        print('Total score (averaged over agents) for episode {}: {}'.format(count ,np.mean(scores)))\n",
    "    scoreList.append([scores])\n",
    "    count += 1\n",
    "    if np.mean(np.array(scoreList)[-100:]) > 30:\n",
    "        break\n",
    "print(\"Total mean score: {}\".format(np.mean(scoreList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 364.123875\n",
    "print(\"test {.2f}\".format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#x = worker.local.fc1(torch.from_numpy(state).float().to(\"cuda\").unsqueeze(0))\n",
    "# nn.BatchNorm1d(128)(x0)\n",
    "worker.local.eval()\n",
    "x = worker.local.fc1(torch.from_numpy(state).float().to(\"cuda\").unsqueeze(0))\n",
    "nn.BatchNorm1d(128)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker.local.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exprewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expstates, Expactions, Exprewards, Expnext_states, Expdones = shared_memory.sample(0)\n",
    "Expnext_actions = workers[0].act(Expnext_states)\n",
    "Expstate, Expaction, Expestimated_reward = critic.evaluate(Expstates.squeeze(), Expactions.squeeze(), Exprewards, Expnext_states.squeeze(), Expnext_actions.squeeze(), Expdones.squeeze())\n",
    "#Expnext_states.squeeze().shape\n",
    "Expnext_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expstates.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO FIX! \n",
    "worker.act(Expnext_states[5], add_noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "critic.target(torch.from_numpy(Expnext_states.squeeze()).float().to(\"cuda\"), torch.from_numpy(Expnext_actions.squeeze()).float().to(\"cuda\")).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for i in range(20):\n",
    "    x = np.array(scoreList)[:,:,i]\n",
    "    plt.plot(range(len(x)), x, label=i)\n",
    "    #plt.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plt.plot(np.array(shared_memory.memory[0])[:,2,0])\n",
    "np.sum((np.array(shared_memory.memory[0])[:,2,0]) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers[0].noise.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expnext_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "state = torch.from_numpy(state).float().to(device)\n",
    "        action = self.local(state).cpu().data.numpy()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return action\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "states0, actions0 = torch.from_numpy(Expstates.squeeze()).float().to(\"cuda\"), torch.from_numpy(Expactions.squeeze()).float().to(\"cuda\")\n",
    "# print(\"states: {}\".format(states))\n",
    "# print(\"actions: {}\".format(actions))\n",
    "Q_expected0 = critic.local(states0, actions0) # .cpu().data.numpy()\n",
    "estimated_rewards0 = torch.from_numpy(Expestimated_reward).float().to(\"cuda\")\n",
    "# print(\"Q_expected: {}\".format(Q_expected))\n",
    "# print(\"estimated_rewards: {}\".format(estimated_rewards))\n",
    "critic_loss = mixed_loss2(Q_expected0, estimated_rewards0)\n",
    "print(critic_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = [0.1*i, 0.1*(i+1)]\n",
    "crit = (estimated_rewards0 >= thres[0]) * (estimated_rewards0 < thres[1])\n",
    "crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expestimated_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_rewards0[estimated_rewards0 > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_loss(expected, from_eval, threshold = 0.01, weigths = [5, 1]):\n",
    "    if len(Q_expected0[estimated_rewards0 >= threshold]) > 0:\n",
    "        a = F.mse_loss(expected[from_eval >= threshold], from_eval[from_eval >= threshold])\n",
    "    else:\n",
    "        a = 0\n",
    "    if len(from_eval[from_eval < threshold]) > 0:\n",
    "        b = F.mse_loss(expected[from_eval < threshold], from_eval[from_eval < threshold])\n",
    "    else:\n",
    "        b = 0\n",
    "    return weigths[0] * a + weigths[1] * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_loss2(expected, from_eval, lim_range = 5000):\n",
    "    \"\"\"\n",
    "    weigthed loss for the critic to give more importance to the high rewards\n",
    "    \"\"\"\n",
    "    cumul = 0\n",
    "    indiv = 0 \n",
    "    for i in range(lim_range):\n",
    "        thres = [0.1*i, 0.1*(i+1)]\n",
    "        crit = (from_eval >= thres[0]) and (from_eval < thres[1])\n",
    "        if len(from_eval[crit]) > 0:\n",
    "            indiv = (i+1) * F.mse_loss(expected[crit], from_eval[crit])\n",
    "        else:\n",
    "            indiv = 0\n",
    "        cumul += indiv\n",
    "    if len(from_eval[from_eval < 0]) > 0:\n",
    "        cumul += F.mse_loss(expected[from_eval < 0], from_eval[from_eval < 0])\n",
    "    if len(from_eval[from_eval > 0.1*(lim_range + 1)]) > 0:\n",
    "        cumul += F.mse_loss(expected[from_eval > 0.1*(lim_range + 1)], from_eval[from_eval > 0.1*(lim_range + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "F.mse_loss(Q_expected0[estimated_rewards0 < threshold], estimated_rewards0[estimated_rewards0 < threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Q_expected0[estimated_rewards0 >= threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
